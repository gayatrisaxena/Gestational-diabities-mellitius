# -*- coding: utf-8 -*-
"""gdm.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QCqgLZyIeEB789X4fin00oMgTpiiVUsT
"""

import numpy as np
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')
import pandas as pd
import seaborn as sns
path = r'/content/drive/MyDrive/Gestational Diabetic Dat Set1.csv'
df = pd.read_csv(path)

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score

"""Explore Data"""

df.head(5)

df.shape

"""Information about all the columns in the Dataset"""

df.info()

df.describe()

"""Data Visulizations"""

plt.figure(figsize=(10, 6))
sns.histplot(df['Age'], bins=20, kde=True)
plt.title('Age Distribution')
plt.xlabel('Age')
plt.ylabel('Frequency')
plt.show()

"""# Data Cleaning

1.Locate Missing Data
"""

df.isnull()

df.duplicated()

df.drop_duplicates()

#df.drop(['Case Number','BMI', 'OGTT'],inplace=True, axis=1)
#df.reset_index(inplace=False)
#df.head()

columns = ['BMI','OGTT','HDL','Sys BP']  # Replace with the actual column names
imputer = SimpleImputer(missing_values=np.nan, strategy='mean')

for column in columns:
  df[column] = imputer.fit_transform(df[[column]])


# calculating correlation
# Calculate the correlation between independent features and the binary output
correlation_with_output = df.corr()['Class Label(GDM /Non GDM)']

# Sort the correlations in descending order
correlation_with_output = correlation_with_output.abs().sort_values(ascending=False)

# Print the features in order of their influence on the output
print("Features Influencing GDM / Non GDM:")
print(correlation_with_output)

#selecting the most prominent features

x=df.iloc[:,[0,15,9,5,12,11,4]].values
y=df.iloc[:,-1].values

# Data Splitting

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=1)
# Feature Scaling
scaler = StandardScaler()
x_train = scaler.fit_transform(x_train)
x_test = scaler.transform(x_test)
print(x_train)
print(x_test)

# Model Building and predicting accuracy
# 1. KNN
from sklearn.neighbors import KNeighborsClassifier
classifier = KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=2)
classifier.fit(x_train, y_train)

# Making Predictions
y_pred = classifier.predict(x_test)

# Creating the Confusion Matrix
cm = confusion_matrix(y_test, y_pred)

print(cm)
acc=np.round(accuracy_score(y_test, y_pred),2)
print('accuracy of KNN',acc*100)

#logistic regression
from sklearn.linear_model import LogisticRegression
classifier= LogisticRegression(random_state=0)
classifier.fit(x_train, y_train)

#Predicting the test set result
y_pred= classifier.predict(x_test)

#Creating the Confusion matrix
from sklearn.metrics import confusion_matrix
cm= confusion_matrix(y_test, y_pred)

print(cm)

#find accuracy

acc=np.round(accuracy_score(y_test,y_pred),2)
print('accuracy  of logistic regression ',acc*100)

#naive bayes
from sklearn.naive_bayes import GaussianNB
classifier = GaussianNB()
classifier.fit(x_train, y_train)

#Predicting the test set result
y_pred= classifier.predict(x_test)

#Creating the Confusion matrix
from sklearn.metrics import confusion_matrix
cm= confusion_matrix(y_test, y_pred)

print(cm)

#find accuracy

acc=np.round(accuracy_score(y_test,y_pred),2)
print('accuracy  of naive bayes ',acc*100)

# svm linear
from sklearn.svm import SVC  # "Support vector classifier"
classifier = SVC(kernel='linear', random_state=0)
classifier.fit(x_train, y_train)

#Predicting the test set result
y_pred= classifier.predict(x_test)

#Creating the Confusion matrix
from sklearn.metrics import confusion_matrix
cm= confusion_matrix(y_test, y_pred)

print(cm)

#find accuracy

acc=np.round(accuracy_score(y_test,y_pred),2)
print('accuracy of linear svm  ',acc*100)

# decision tree
from sklearn.tree import DecisionTreeClassifier
classifier= DecisionTreeClassifier(criterion='entropy', random_state=0)
classifier.fit(x_train, y_train)

#Predicting the test set result
y_pred= classifier.predict(x_test)

#Creating the Confusion matrix
from sklearn.metrics import confusion_matrix
cm= confusion_matrix(y_test, y_pred)

print(cm)

#find accuracy

acc=np.round(accuracy_score(y_test,y_pred),2)
print('accuracy  of decision tree ',acc*100)

# random forest
from sklearn.ensemble import RandomForestClassifier
classifier= RandomForestClassifier(n_estimators= 10, criterion="entropy")
classifier.fit(x_train, y_train)

#Predicting the test set result
y_pred= classifier.predict(x_test)
#Creating the Confusion matrix
from sklearn.metrics import confusion_matrix
cm= confusion_matrix(y_test, y_pred)

print(cm)

#find accuracy

acc=np.round(accuracy_score(y_test,y_pred),2)
print('accuracy  of random forest  ',acc*100)